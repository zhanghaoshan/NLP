{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParserModel(nn.Module):\n",
    "    \"\"\" Feedforward neural network with an embedding layer and single hidden layer.\n",
    "    The ParserModel will predict which transition should be applied to a\n",
    "    given partial parse configuration.\n",
    "\n",
    "    PyTorch Notes:\n",
    "        - Note that \"ParserModel\" is a subclass of the \"nn.Module\" class. In PyTorch all neural networks\n",
    "            are a subclass of this \"nn.Module\".\n",
    "        - The \"__init__\" method is where you define all the layers and their respective parameters\n",
    "            (embedding layers, linear layers, dropout layers, etc.).\n",
    "        - \"__init__\" gets automatically called when you create a new instance of your class, e.g.\n",
    "            when you write \"m = ParserModel()\".\n",
    "        - Other methods of ParserModel can access variables that have \"self.\" prefix. Thus,\n",
    "            you should add the \"self.\" prefix layers, values, etc. that you want to utilize\n",
    "            in other ParserModel methods.\n",
    "        - For further documentation on \"nn.Module\" please see https://pytorch.org/docs/stable/nn.html.\n",
    "    \"\"\"\n",
    "    def __init__(self, embeddings, n_features=36,\n",
    "        hidden_size=200, n_classes=3, dropout_prob=0.5):\n",
    "        \"\"\" Initialize the parser model.\n",
    "\n",
    "        @param embeddings (Tensor): word embeddings (num_words, embedding_size)\n",
    "        @param n_features (int): number of input features\n",
    "        @param hidden_size (int): number of hidden units\n",
    "        @param n_classes (int): number of output classes\n",
    "        @param dropout_prob (float): dropout probability\n",
    "        \"\"\"\n",
    "        super(ParserModel, self).__init__()\n",
    "        self.n_features = n_features\n",
    "        self.n_classes = n_classes\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.embed_size = embeddings.shape[1]\n",
    "        self.hidden_size = hidden_size\n",
    "        self.pretrained_embeddings = nn.Embedding(embeddings.shape[0], self.embed_size)\n",
    "        self.pretrained_embeddings.weight = nn.Parameter(torch.tensor(embeddings))\n",
    "        \n",
    "        #nn.Embedding()，讲embedding保存为一个字典，方便查找对应的embedding\n",
    "        #nn.Parameter(),类似于类型抓换，讲定值的tensor转换为可以改变、训练的参数\n",
    "        \n",
    " \n",
    "\n",
    "        ### YOUR CODE HERE (~5 Lines)\n",
    "        ### TODO:\n",
    "        ###     1) Construct `self.embed_to_hidden` linear layer, initializing the weight matrix\n",
    "        ###         with the `nn.init.xavier_uniform_` function with `gain = 1` (default)\n",
    "        ###     2) Construct `self.dropout` layer.\n",
    "        ###     3) Construct `self.hidden_to_logits` linear layer, initializing the weight matrix\n",
    "        ###         with the `nn.init.xavier_uniform_` function with `gain = 1` (default)\n",
    "        ###\n",
    "        ### Note: Here, we use Xavier Uniform Initialization（均匀初始化） for our Weight initialization.\n",
    "        ###         It has been shown empirically, that this provides better initial weights\n",
    "        ###         for training networks than random uniform initialization.\n",
    "        ###         For more details checkout this great blogpost:\n",
    "        ###             http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization \n",
    "        ### Hints:\n",
    "        ###     - After you create a linear layer you can access the weight\n",
    "        ###       matrix via:\n",
    "        ###         linear_layer.weight\n",
    "        ###\n",
    "        ### Please see the following docs for support:\n",
    "        ###     Linear Layer: https://pytorch.org/docs/stable/nn.html#torch.nn.Linear\n",
    "        ###     Xavier Init: https://pytorch.org/docs/stable/nn.html#torch.nn.init.xavier_uniform_\n",
    "        ###     Dropout: https://pytorch.org/docs/stable/nn.html#torch.nn.Dropout\n",
    "        \n",
    "        #embed->hidden\n",
    "        self.embed_to_hidden=nn.Linear(self.n_features*self.embed_size,self.hidden_size)\n",
    "        nn.init.xaview_uniform_(self.embed_to_hidden.weight,gain=1)\n",
    "        \n",
    "        #dropout layer\n",
    "        self.dropout=nn.Dropout(self.dropout_prob)\n",
    "        \n",
    "        #output layer\n",
    "        self.hidden_to_logits=nn.Linear(self.hidden_size,self.n_classes)\n",
    "        nn.init.xavier_uniform_(self.hiidden_to_logits.weight,gain=1)\n",
    "        ### END YOUR CODE\n",
    "        \n",
    "            \n",
    "\n",
    "    def embedding_lookup(self, t):\n",
    "        \"\"\" Utilize `self.pretrained_embeddings` to map input `t` from input tokens (integers)\n",
    "            to embedding vectors.\n",
    "\n",
    "            PyTorch Notes:\n",
    "                - `self.pretrained_embeddings` is a torch.nn.Embedding object that we defined in __init__\n",
    "                - Here `t` is a tensor where each row represents a list of features. Each feature is represented by an integer (input token).\n",
    "                - In PyTorch the Embedding object, e.g. `self.pretrained_embeddings`, allows you to\n",
    "                    go from an index to embedding. Please see the documentation (https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding)\n",
    "                    to learn how to use `self.pretrained_embeddings` to extract the embeddings for your tensor `t`.\n",
    "\n",
    "            @param t (Tensor): input tensor of tokens (batch_size, n_features)\n",
    "\n",
    "            @return x (Tensor): tensor of embeddings for words represented in t\n",
    "                                (batch_size, n_features * embed_size)\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE (~1-3 Lines)\n",
    "        ### TODO:\n",
    "        ###     1) Use `self.pretrained_embeddings` to lookup the embeddings for the input tokens in `t`.\n",
    "        ###     2) After you apply the embedding lookup, you will have a tensor shape (batch_size, n_features, embedding_size).\n",
    "        ###         Use the tensor `view` method to reshape the embeddings tensor to (batch_size, n_features * embedding_size)\n",
    "        ###\n",
    "        ### Note: In order to get batch_size, you may need use the tensor .size() function:\n",
    "        ###         https://pytorch.org/docs/stable/tensors.html#torch.Tensor.size\n",
    "        ###\n",
    "        ###  Please see the following docs for support:\n",
    "        ###     Embedding Layer: https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding\n",
    "        ###     View: https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view\n",
    "        x=self.pretrained_embeddings(t)\n",
    "        x=x.veiw(x.shape[0],x.shape[1]*x.shape[2])\n",
    "\n",
    "        ### END YOUR CODE\n",
    "        return x\n",
    "\n",
    "\n",
    "    def forward(self, t):\n",
    "        \"\"\" Run the model forward.\n",
    "\n",
    "            Note that we will not apply the softmax function here because it is included in the loss function nn.CrossEntropyLoss\n",
    "\n",
    "            PyTorch Notes:\n",
    "                - Every nn.Module object (PyTorch model) has a `forward` function.\n",
    "                - When you apply your nn.Module to an input tensor `t` this function is applied to the tensor.\n",
    "                    For example, if you created an instance of your ParserModel and applied it to some `t` as follows,\n",
    "                    the `forward` function would called on `t` and the result would be stored in the `output` variable:\n",
    "                        model = ParserModel()\n",
    "                        output = model(t) # this calls the forward function\n",
    "                - For more details checkout: https://pytorch.org/docs/stable/nn.html#torch.nn.Module.forward\n",
    "\n",
    "        @param t (Tensor): input tensor of tokens (batch_size, n_features)\n",
    "\n",
    "        @return logits (Tensor): tensor of predictions (output after applying the layers of the network)\n",
    "                                 without applying softmax (batch_size, n_classes)\n",
    "        \"\"\"\n",
    "        ###  YOUR CODE HERE (~3-5 lines)\n",
    "        ### TODO:\n",
    "        ###     1) Apply `self.embedding_lookup` to `t` to get the embeddings\n",
    "        ###     2) Apply `embed_to_hidden` linear layer to the embeddings\n",
    "        ###     3) Apply relu non-linearity to the output of step 2 to get the hidden units.\n",
    "        ###     4) Apply dropout layer to the output of step 3.\n",
    "        ###     5) Apply `hidden_to_logits` layer to the output of step 4 to get the logits.\n",
    "        ###\n",
    "        ### Note: We do not apply the softmax to the logits here, because\n",
    "        ### the loss function (torch.nn.CrossEntropyLoss) applies it more efficiently.\n",
    "        ###\n",
    "        ### Please see the following docs for support:\n",
    "        ###     ReLU: https://pytorch.org/docs/stable/nn.html?highlight=relu#torch.nn.functional.relu\n",
    "        x=embedding_lookup(t)\n",
    "        h = self.embed_to_hidden(a)\n",
    "        h = F.relu(h)\n",
    "        h = self.dropout(h)\n",
    "        logits = self.hidden_to_logits(h)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        ### END YOUR CODE\n",
    "        return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## embedding_lookup()中间还有部分问题需要看看pytorch的先关教程\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([], size=(0, 5), grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "\n",
    "word_to_ix = {'hello': 0, 'world': 1}\n",
    "embeds = nn.Embedding(2, 5)\n",
    "hello_idx = torch.LongTensor(0)\n",
    "hello_idx = Variable(hello_idx)\n",
    "hello_embed = embeds(hello_idx)\n",
    "print(hello_embed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
