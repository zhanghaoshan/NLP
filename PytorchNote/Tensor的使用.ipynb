{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 注意事项\n",
    "1 tensor类似python中的不可变变量，所以类似a.view(1,4)执行后，a本身是不会产生变化的。\n",
    "\n",
    "2 部分tensor的方法可以执行tensor的改变，例如x.add_(y),即为x=x+y，但是普通的x.add(y)是无法改变x本身的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 tensor的初始化\n",
    "\n",
    "1直接进行赋值。a=t.Tensor([1,2,4,5])\n",
    "\n",
    "2a=t.rand(shape)\n",
    "\n",
    "3a=t.zeros(shape)\n",
    "\n",
    "4a=t.ones(shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 tensor中的乘法\n",
    "1tensor中*是对应元素相乘，或者其他的数组计算，总之不遵守矩阵乘法规则\n",
    "\n",
    "2tensor中torch.mm()是专门用来矩阵乘法的。\n",
    "\n",
    "3tensor中矩阵和数组是两个概念，也没有列向量的概念。tensor无法表示为列向量，只能表示为一个人4*1的矩阵.\n",
    "\n",
    "例如，a=t.rand(4),b=t.rand(4,1),c=t.mm(a,b)，这时c是无法计算的，应为一维tensor无法和二维tensor做矩阵乘法，应该把a改为a=t.rand(1,4)。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 tensor中的变形操作\n",
    "1 t.reshape()，可以应用于任意维度。例如行向量转4*1矩阵\n",
    "\n",
    "2 t.t(),只能应用于二维tensor的转置\n",
    "\n",
    "3 torch.transpose(input, dim0, dim1)：交换两个维度。可以应用于任意维度的tensor。常用语图片的旋转。\n",
    "\n",
    "4 t.squeeze(input)：去除那些维度大小为1的维度 \n",
    "\n",
    "5 还有一个tensor自己的方法，用来改变自身大小，view\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1],\n",
      "        [2],\n",
      "        [4],\n",
      "        [4]])\n",
      "tensor([1, 2, 4, 4])\n",
      "tensor([[-2.5569e-26,  4.5915e-41,  1.4612e+23,  7.6791e-43]])\n",
      "tensor([[-2.5569e-26],\n",
      "        [ 4.5915e-41],\n",
      "        [ 1.4612e+23],\n",
      "        [ 7.6791e-43]])\n"
     ]
    }
   ],
   "source": [
    "# squeeze的应用\n",
    "a=t.tensor([1,2,4,4])\n",
    "a=t.reshape(a,[4,1])\n",
    "print(a)\n",
    "a=t.squeeze(a)\n",
    "print(a)\n",
    "\n",
    "#view的应用\n",
    "b=t.Tensor(1,4)\n",
    "print(b)\n",
    "b=b.view(-1,1)\n",
    "print(b)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 tensor中的组合操作\n",
    "1 torch.cat(seq, dim=0, out=None)(concatenate,组合） "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3289, 0.7240, 0.1107, 0.7279, 0.4546],\n",
      "        [0.7002, 0.4043, 0.9092, 0.5928, 0.5989],\n",
      "        [0.6032, 0.0133, 0.2696, 0.7771, 0.2774],\n",
      "        [0.9897, 0.9710, 0.6988, 0.8683, 0.9272],\n",
      "        [0.6534, 0.5714, 0.9044, 0.9126, 0.2624]])\n",
      "tensor([[0.7640, 0.0811, 0.0388, 0.0842, 0.4306, 0.6032, 0.0133, 0.2696, 0.7771,\n",
      "         0.2774],\n",
      "        [0.5613, 0.9848, 0.8707, 0.6648, 0.8495, 0.9897, 0.9710, 0.6988, 0.8683,\n",
      "         0.9272],\n",
      "        [0.2767, 0.3555, 0.1163, 0.1815, 0.2954, 0.6534, 0.5714, 0.9044, 0.9126,\n",
      "         0.2624]])\n"
     ]
    }
   ],
   "source": [
    "x1 = t.rand(2, 5)\n",
    "y1 = t.rand(3, 5)\n",
    "z1 = t.cat([x1, y1])\n",
    "z2=t.cat([x1,y1],0)\n",
    "print(z2)\n",
    "# z3=t.cat([x1,y1],1)，这个操作就不对了，因为如果按照dim=1组合的话，x1只有两行，不能组合到y1上去。\n",
    "x2=t.rand(3,5)\n",
    "z3=t.cat([x2,y1],1)\n",
    "print(z3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5tensor中的数学操作\n",
    "1 求取所有元素总和。tensor.sum()\n",
    "\n",
    "2 求取所有元素平均值，tensor.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "tensor(12.)\n",
      "tensor(1.)\n",
      "torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "a=t.ones(3,4)\n",
    "print(a)\n",
    "print(a.sum())\n",
    "print(a.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1])\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# 1 size的使用\n",
    "a=t.Tensor(4,1)\n",
    "# print(t.size(a)),这种会报错，torch中没有size函数，size是tensor自己的方法。\n",
    "print(a.size())\n",
    "print(a.size()[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 tensor的梯度计算\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "None\n",
      "tensor(16., grad_fn=<SumBackward0>)\n",
      "x.grad= tensor([[8., 8.],\n",
      "        [8., 8.]])\n",
      "y.grad= None\n",
      "z.grad= None\n"
     ]
    }
   ],
   "source": [
    "# 1修改requires_grad属性\n",
    "a=t.ones(2,3,requires_grad=True)\n",
    "a=t.Tensor([1,3,4])\n",
    "a.requires_grad=True\n",
    "print(a.requires_grad)\n",
    "\n",
    "# 2初始的tensor不能对自身进行运算，否则无法求梯度\n",
    "x=t.ones(2,2,requires_grad=True)\n",
    "x=x*2\n",
    "y=x.sum()\n",
    "y.backward()\n",
    "print(x.grad)\n",
    "\n",
    "#3 只能是标量对向量求梯度，无法求向量对向量的梯度\n",
    "x=t.ones(2,2,requires_grad=True)\n",
    "y=2*x\n",
    "# y.backward()报错，无法求向量对向量的梯度\n",
    "\n",
    "# 4只能求对初始tensor的梯度\n",
    "x=t.ones(2,2,requires_grad=True)\n",
    "y=2*x\n",
    "z=y*y\n",
    "out=z.sum()\n",
    "print(out)\n",
    "out.backward()\n",
    "print('x.grad=',x.grad)\n",
    "print('y.grad=',y.grad)\n",
    "print('z.grad=',z.grad)\n",
    "\n",
    "# 5可以使用雅克比向量积的方式通过链式求导法则计算向量对向量的倒数。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
