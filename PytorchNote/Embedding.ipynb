{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1python中的embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.8328,  0.4381, -0.9808, -0.7558,  0.7410, -0.1233,  0.6379,  0.9327,\n",
      "         0.9399,  2.2820], grad_fn=<AsStridedBackward>)\n"
     ]
    }
   ],
   "source": [
    "word2idx={'hello':0,'world':1} \n",
    "embeds=nn.Embedding(2,5)\n",
    "#lookup_tensor=torch.Tensor([word2idx['hello']],dtype=torch.LongTensor)\n",
    "lookup_tensor = torch.LongTensor([word2idx[\"hello\"]])\n",
    "# 新建立的tensor一定是torch.tensor([]),[]不要忘了\n",
    "hello_embed=embeds(lookup_tensor)\n",
    "# hello_embed的输入，一定是一个int，一般都用longtensor\n",
    "inputs=torch.LongTensor([0,1])\n",
    "embed=embeds(inputs).view(1,-1)  #经过view变换后，仍然是一个矩阵，1*n，如果要变成向量，还需要flatten\n",
    "embed=embed.flatten()\n",
    "print(embed)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1装备数据和word2idx\n",
    "test_sentence = \"\"\"When forty winters shall besiege thy brow,\n",
    "And dig deep trenches in thy beauty's field,\n",
    "Thy youth's proud livery so gazed on now,\n",
    "Will be a totter'd weed of small worth held:\n",
    "Then being asked, where all thy beauty lies,\n",
    "Where all the treasure of thy lusty days;\n",
    "To say, within thine own deep sunken eyes,\n",
    "Were an all-eating shame, and thriftless praise.\n",
    "How much more praise deserv'd thy beauty's use,\n",
    "If thou couldst answer 'This fair child of mine\n",
    "Shall sum my count, and make my old excuse,'\n",
    "Proving his beauty by succession thine!\n",
    "This were to be new made when thou art old,\n",
    "And see thy blood warm when thou feel'st it cold.\"\"\".split()\n",
    "\n",
    "EMBEDDING_DIM=10\n",
    "CONTEXT_SIZE=2\n",
    "VOCAB_SIZE=len(vocab)\n",
    "\n",
    "trigrams=[([test_sentence[i-1],test_sentence[i]],test_sentence[i+1]) for i in range(1,len(test_sentence)-1)]\n",
    "vocab=list(set(test_sentence))\n",
    "word2idx={word:i for i,word in enumerate(vocab)}\n",
    "#for context,target in trigrams:\n",
    "    #print('context:{},target:{}'.format(context,target))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2建立模型\n",
    "class nGramModel(nn.Module):\n",
    "    def __init__(self,embedding_dim,context_size,vocab_size):\n",
    "        super(nGramModel,self).__init__()\n",
    "        self.embeddings=nn.Embedding(vocab_size,embedding_dim)\n",
    "        self.linear1=nn.Linear(context_size*embedding_dim,128)\n",
    "        self.linear2=nn.Linear(128,vocab_size)\n",
    "    def forward(self,inputs):\n",
    "        embeds=self.embeddings(inputs).view(1,-1)\n",
    "        out=self.linear1(embeds)\n",
    "        out=F.relu(out)\n",
    "        out=self.linear2(out)\n",
    "        #print(out)\n",
    "        log_prob=F.log_softmax(out,dim=1)\n",
    "        #注意，这里dim描述的是out的形式，如果dim输入不正确，log_prob可能计算为0矩阵。\n",
    "        #print(log_prob)\n",
    "        return log_prob\n",
    "model=nGramModel(EMBEDDING_DIM,CONTEXT_SIZE,VOCAB_SIZE)\n",
    "    \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 关于model的训练\n",
    "1 model=nGramModel(EMBEDDING_DIM,CONTEXT_SIZE,VOCAB_SIZE)这句话，实际上是在执行构造函数\n",
    "\n",
    "2执行完构造函数后，model就成为了forward的引用，之后model就是forward函数。所以要计算前向结果，直接model（input）\n",
    "\n",
    "3nn.NLLLoss()的输入很奇怪，同时需要总结一下其他非线性函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch0: loss=50.0132942199707\n",
      "Epoch1: loss=46.42011642456055\n",
      "Epoch2: loss=40.404701232910156\n",
      "Epoch3: loss=38.716468811035156\n",
      "Epoch4: loss=34.52384948730469\n",
      "Epoch5: loss=33.47443389892578\n",
      "Epoch6: loss=30.295167922973633\n",
      "Epoch7: loss=29.472652435302734\n",
      "Epoch8: loss=26.973745346069336\n",
      "Epoch9: loss=26.282020568847656\n",
      "Epoch10: loss=24.347497940063477\n",
      "Epoch11: loss=23.791004180908203\n",
      "Epoch12: loss=22.35445785522461\n",
      "Epoch13: loss=21.885984420776367\n",
      "Epoch14: loss=20.841094970703125\n",
      "Epoch15: loss=20.42546844482422\n",
      "Epoch16: loss=19.641998291015625\n",
      "Epoch17: loss=19.22689437866211\n",
      "Epoch18: loss=18.605792999267578\n",
      "Epoch19: loss=18.18592071533203\n",
      "Epoch20: loss=17.682117462158203\n",
      "Epoch21: loss=17.249794006347656\n",
      "Epoch22: loss=16.7958984375\n",
      "Epoch23: loss=16.381576538085938\n",
      "Epoch24: loss=15.954948425292969\n",
      "Epoch25: loss=15.55398178100586\n",
      "Epoch26: loss=15.157174110412598\n",
      "Epoch27: loss=14.772966384887695\n",
      "Epoch28: loss=14.396240234375\n",
      "Epoch29: loss=14.030515670776367\n",
      "Epoch30: loss=13.675812721252441\n",
      "Epoch31: loss=13.329268455505371\n",
      "Epoch32: loss=13.00212574005127\n",
      "Epoch33: loss=12.673514366149902\n",
      "Epoch34: loss=12.360884666442871\n",
      "Epoch35: loss=12.055322647094727\n",
      "Epoch36: loss=11.758686065673828\n",
      "Epoch37: loss=11.478804588317871\n",
      "Epoch38: loss=11.207111358642578\n",
      "Epoch39: loss=10.947712898254395\n",
      "Epoch40: loss=10.695439338684082\n",
      "Epoch41: loss=10.453723907470703\n",
      "Epoch42: loss=10.22098159790039\n",
      "Epoch43: loss=9.997323989868164\n",
      "Epoch44: loss=9.780926704406738\n",
      "Epoch45: loss=9.576310157775879\n",
      "Epoch46: loss=9.375791549682617\n",
      "Epoch47: loss=9.183941841125488\n",
      "Epoch48: loss=8.995755195617676\n",
      "Epoch49: loss=8.815292358398438\n",
      "Epoch50: loss=8.637266159057617\n",
      "Epoch51: loss=8.466276168823242\n",
      "Epoch52: loss=8.303315162658691\n",
      "Epoch53: loss=8.146892547607422\n",
      "Epoch54: loss=7.997343063354492\n",
      "Epoch55: loss=7.852789402008057\n",
      "Epoch56: loss=7.714898109436035\n",
      "Epoch57: loss=7.582893371582031\n",
      "Epoch58: loss=7.457554817199707\n",
      "Epoch59: loss=7.338011741638184\n",
      "Epoch60: loss=7.225497722625732\n",
      "Epoch61: loss=7.1183600425720215\n",
      "Epoch62: loss=7.017017841339111\n",
      "Epoch63: loss=6.921850204467773\n",
      "Epoch64: loss=6.83184814453125\n",
      "Epoch65: loss=6.747335433959961\n",
      "Epoch66: loss=6.667848587036133\n",
      "Epoch67: loss=6.593726634979248\n",
      "Epoch68: loss=6.523665904998779\n",
      "Epoch69: loss=6.462282180786133\n",
      "Epoch70: loss=6.400126934051514\n",
      "Epoch71: loss=6.341799259185791\n",
      "Epoch72: loss=6.284844398498535\n",
      "Epoch73: loss=6.233349800109863\n",
      "Epoch74: loss=6.185146808624268\n",
      "Epoch75: loss=6.139967918395996\n",
      "Epoch76: loss=6.097021102905273\n",
      "Epoch77: loss=6.058259010314941\n",
      "Epoch78: loss=6.02004337310791\n",
      "Epoch79: loss=5.98338508605957\n",
      "Epoch80: loss=5.948303699493408\n",
      "Epoch81: loss=5.9156341552734375\n",
      "Epoch82: loss=5.884764671325684\n",
      "Epoch83: loss=5.855610370635986\n",
      "Epoch84: loss=5.826925754547119\n",
      "Epoch85: loss=5.8011250495910645\n",
      "Epoch86: loss=5.775620460510254\n",
      "Epoch87: loss=5.751171112060547\n",
      "Epoch88: loss=5.7277727127075195\n",
      "Epoch89: loss=5.706485748291016\n",
      "Epoch90: loss=5.684946060180664\n",
      "Epoch91: loss=5.667880535125732\n",
      "Epoch92: loss=5.648200988769531\n",
      "Epoch93: loss=5.629157066345215\n",
      "Epoch94: loss=5.607855319976807\n",
      "Epoch95: loss=5.589813232421875\n",
      "Epoch96: loss=5.572678565979004\n",
      "Epoch97: loss=5.5560407638549805\n",
      "Epoch98: loss=5.540113925933838\n",
      "Epoch99: loss=5.526002407073975\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def train(model):\n",
    "    loss_function = nn.NLLLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "    for epoch in range(100):\n",
    "        loss_list=[]\n",
    "        model.zero_grad()\n",
    "        for context,label in trigrams:\n",
    "            inputs=torch.LongTensor([word2idx[context[0]],word2idx[context[1]]])\n",
    "            target=torch.LongTensor([word2idx[label]])\n",
    "            #这里，还需要仔细了解nn.NLLLoss，这个loss函数的输入时一个tensor向量，一个标量，很奇怪。\n",
    "            log_prob=model(inputs)\n",
    "            loss=loss_function(log_prob,target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_list.append(loss)\n",
    "        loss_total=sum(loss_list)\n",
    "        print('Epoch{}: loss={}'.format(epoch,loss_total))\n",
    "\n",
    "        \n",
    "train(model)  \n",
    "\n",
    "\n",
    "\n",
    "         \n",
    "            \n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test result: loss= tensor(4.1054, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss_function = nn.NLLLoss()\n",
    "loss_list=[]\n",
    "for context,label in trigrams:\n",
    "    inputs=torch.LongTensor([word2idx[context[0]],word2idx[context[1]]])\n",
    "    target=torch.LongTensor([word2idx[label]])       \n",
    "    log_prob=model(inputs)\n",
    "    loss=loss_function(log_prob,target)\n",
    "    loss_list.append(loss)\n",
    "loss_total=sum(loss_list)\n",
    "print(\"Test result: loss=\",loss_total)\n",
    "#再一次证明，在training函数中训练model后，model的参数却是改变了，不用担心model是可变变量还是不可变变量"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
